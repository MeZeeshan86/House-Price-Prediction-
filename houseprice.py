# -*- coding: utf-8 -*-
"""HousePrice.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17ZoBb366vQ94lDzb5fRvuhyRq_FGBueI
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

"""Importing the dataset"""

df=pd.read_csv('housing.csv')

"""Analyzing the data"""

df.sample(7)

df.describe()

df.isnull().sum()

df.dropna(inplace=True)

df.info()

"""Splitting the data into Train & Test"""

from sklearn.model_selection import train_test_split
x=df.drop('median_house_value',axis=1)
y=df['median_house_value']
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)

train_data=pd.concat([x_train,y_train],axis=1)
test_data=pd.concat([x_test,y_test],axis=1)

"""Encoding the ocean_proximity column"""

df.ocean_proximity.value_counts()

train_data=train_data.join(pd.get_dummies(train_data.ocean_proximity)).drop(['ocean_proximity'],axis=1)
test_data=test_data.join(pd.get_dummies(test_data.ocean_proximity)).drop(['ocean_proximity'],axis=1)

"""Understanding Training Data"""

train_data.hist(figsize=(15,15))

"""Transforming and feature engineering


"""

train_data['total_rooms'] = np.log(train_data['total_rooms'] + 1)
train_data['total_bedrooms'] = np.log(train_data['total_bedrooms'] + 1)
train_data['population'] = np.log(train_data['population'] + 1)
train_data['households'] = np.log(train_data['households'] + 1)
train_data['median_income'] = np.log(train_data['median_income'] + 1)
train_data['bedroom_ratio'] = train_data['total_bedrooms'] / train_data['total_rooms']
train_data['household_rooms'] = train_data['total_rooms'] / train_data['households']


test_data['total_rooms'] = np.log(test_data['total_rooms'] + 1)
test_data['total_bedrooms'] = np.log(test_data['total_bedrooms'] + 1)
test_data['population'] = np.log(test_data['population'] + 1)
test_data['households'] = np.log(test_data['households'] + 1)
test_data['median_income'] = np.log(test_data['median_income'] + 1)
test_data['bedroom_ratio'] = test_data['total_bedrooms'] / test_data['total_rooms']
test_data['household_rooms'] = test_data['total_rooms'] / test_data['households']

"""Understanding the corr using heatmap"""

plt.figure(figsize=(20,15))
sns.heatmap(train_data.corr(),annot=True,cmap='YlGnBu')

plt.figure(figsize=(15,8))
sns.scatterplot(x='longitude',y='latitude',hue='median_house_value',data=train_data,palette='coolwarm')

"""Splitting X and Y columns"""

x_test,y_test = test_data.drop('median_house_value',axis=1),test_data['median_house_value']
x_train,y_train = train_data.drop('median_house_value',axis=1),train_data['median_house_value']
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()

x_train_scaled = scaler.fit_transform(x_train)
x_test_scaled = scaler.transform(x_test)

"""Using Random Forest Model and running GridSearCV"""

from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestRegressor

forest = RandomForestRegressor()

param_grid = {
    "n_estimators": [100, 200, 300],
    "min_samples_split": [2, 4],
    "max_depth": [None, 4, 8],
}


grid_search = GridSearchCV(
    forest,
    param_grid,
    cv=5,
    scoring="neg_mean_squared_error",
    return_train_score=True
)


grid_search.fit(x_train_scaled, y_train)

print("Best parameters:", grid_search.best_params_)
best_forest = grid_search.best_estimator_
#Best parameters: {'max_depth': None, 'min_samples_split': 4, 'n_estimators': 200}

from sklearn.metrics import mean_squared_error, r2_score

y_pred = best_forest.predict(x_test_scaled)

mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
rmse = np.sqrt(mse)
print("Test MSE:", mse)
print("Test R^2:", r2)
print("Test RMSE:", rmse)

"""Saving the model in Pickle"""

import pickle
with open('House_price.pkl', 'wb') as file:
    pickle.dump(best_forest, file)